Metadata-Version: 2.4
Name: langmesh-openai
Version: 1.0.0
Summary: langmesh-wrapped OpenAI client - drop-in replacement with telemetry and cost optimization
Author-email: langmesh <support@langmesh.ai>
License: MIT
Project-URL: Homepage, https://langmesh.ai
Project-URL: Repository, https://github.com/langmesh-ai/openai-python
Project-URL: Documentation, https://docs.langmesh.ai
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: openai>=1.0.0
Requires-Dist: httpx>=0.25.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-mock>=3.11.1; extra == "dev"

# langmesh-openai

Drop-in replacement for the OpenAI Python client with automatic cost optimization and telemetry.

## Installation

```bash
pip install langmesh-openai
```

## Usage

Change one line of code:

```python
# Before
from openai import OpenAI

# After
from langmesh_openai import OpenAI

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# Everything works exactly the same!
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

That's it. No configuration needed.

## What It Does

### Telemetry (Always On)

- Tracks token usage, cost, and latency
- Privacy-preserving (no prompts sent by default)
- Zero performance impact (async)
- Never breaks your app (fail-safe)

### Cost Optimization (Opt-In)

When you enable policies in the langmesh dashboard:

- Automatic model downgrading for simple queries
- Retry storm suppression
- Exact Cache (identical requests)
- Semantic Deduplication (high-threshold reuse)
- Semantic Answer Cache (advanced, opt-in)
- Token optimization

## Configuration

### Required

```bash
export langmesh_API_KEY=sk_live_...  # Get from dashboard.langmesh.ai
```

### Optional

```bash
export langmesh_PROXY_ENABLED=true  # Enable when policies require routing
export langmesh_BASE_URL=https://api.langmesh.ai/v1/openai  # Custom proxy URL
```

## Async Support

```python
from langmesh_openai import AsyncOpenAI

client = AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])

response = await client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## Migration Path

1. **Install** - `pip install langmesh-openai`
2. **Replace import** - Change `from openai` to `from langmesh_openai`
3. **Set API key** - `export langmesh_API_KEY=sk_live_...`
4. **See savings** - Visit dashboard.langmesh.ai
5. **Enable policies** - When ready, `export langmesh_PROXY_ENABLED=true`

## Guarantees

✅ Drop-in replacement - works identically
✅ No behavior changes without opt-in
✅ Fail-safe - errors don't break your app
✅ Reversible - uninstall anytime
✅ Privacy-first - no prompts sent by default

## License

MIT
